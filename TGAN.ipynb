{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 《Temporal Generative Adversarial Nets》\n",
    "\n",
    "[paper link](https://arxiv.org/abs/1611.06624)\n",
    "\n",
    "## work\n",
    "\n",
    "一个生成模型（Temproal GAN），可以学习无标签视频的语义表示（semantic representation），还可以生成连续的视频。\n",
    "\n",
    "## model\n",
    "\n",
    "![](https://raw.githubusercontent.com/applenob/paper_note/master/res/tgan-model.png)\n",
    "\n",
    "模型由两个子模型构成：1.**temporal generator**：生成一系列和image generator的潜在变量相关的潜在变量，由1D的deconvolutional层组成；2.**image generator**：将上面一步生成的潜在变量转换成视频的每一帧，由2D的deconvolutional层组成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "T是生成的视频帧数；\n",
    "\n",
    "temporal generator：$G_0：R^{K_0} \\rightarrow R^{T×K_1}$；\n",
    "\n",
    "$z_0 ∈ R^{K_0}$，另外$z_0 \\sim p_{G_0}(z_0)$，是$G_0$的随机噪声变量，也可以称为是潜在变量（latent variable）；\n",
    "\n",
    "$z_1 ∈ R^{K_1}$，$z_1$是$G_1$的潜在变量，但是不是从一个分布中随机抽样，而是来自$G_0$;\n",
    "\n",
    "$G_0$生成了T个潜在变量：$[z^1_1, z^2_1, ..., z^T_1]$;\n",
    "\n",
    "$G_1$再用这些变量生成T帧图像$[G_1(z^1_1), G_1(z^2_1), ..., G_1(z^T_1)]$，其中，$G_1：R^{K_1}\\rightarrow R^{M}$\n",
    "\n",
    "$D_1'：R^M\\rightarrow R^F$（temporal discriminator）：从每一帧图像中提取F维的特征向量。\n",
    "\n",
    "$D_0：R^{T×F}\\rightarrow [0,1]$（image discriminator）：判别输入是来自dataset还是generator。\n",
    "\n",
    "目标函数：\n",
    "\n",
    "$$\\underset{θ_{G_0},θ_{G_1}}{min}\\underset{θ_{D_0},θ_{D_1}}{max}\\mathbb{E}_{[x^1, ..., x^T]\\sim p_{video}}[lnD_0([D_1'(x^1), ..., D_1'(x^T)])]+\\mathbb{E}_{z_0\\sim p_{G_0}}[ln(1-D_0([D_1'(x^1), ..., D_1'(x^T)]))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train\n",
    "\n",
    "1.先使用下式对$G_1$和$D_1$进行训练：\n",
    "\n",
    "$$\\underset{θ_{G_1}}{min}\\underset{θ_{D_1}}{max}\\mathbb{E}_{x\\sim p_{image}}[lnD_1(x^1)]+\\mathbb{E}_{z_1\\sim p_{G_1}}[ln(1-D_1(G_1(z_1))]$$\n",
    "\n",
    "获得参数：$θ_{G_1}$和$θ_{D_1}$；\n",
    "\n",
    "2.再用目标函数训练$G_0$和$D_0$。其中，$D_1'$的训练参数使用第一步训练的$D_1$的结果，但是loss layer要改成提取一个特征向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
